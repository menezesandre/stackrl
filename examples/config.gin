# Set environment parameters

# stack.register.episode_length=32
# stack.register.urdfs='train'
# stack.register.object_max_dimension=0.125
# stack.register.use_gui=False
# stack.register.sim_time_step=1/60.
# stack.register.gravity=9.8
# stack.register.num_sim_steps=None
# stack.register.velocity_threshold=0.01
# stack.register.smooth_placing = True
# stack.register.observable_size_ratio=4
# stack.register.resolution_factor=5
# stack.register.max_z=1
# stack.register.goal_size_ratio=.375
# stack.register.occupation_ratio_weight=0.
# stack.register.occupation_ratio_param=False
# stack.register.positions_weight=0.
# stack.register.positions_param=0.
# stack.register.n_steps_weight=0.
# stack.register.n_steps_param=0.
# stack.register.contact_points_weight=0.
# stack.register.contact_points_param=0.
# stack.register.differential=True
# stack.register.flat_action=True
# stack.register.dtype='float32'

# stack.curriculum.goals = []
# stack.curriculum.ckwargs = {}

# Set network parameters

# left/nets.sequential.layers = [
#   (@tf.keras.layers.Conv2D,
#     {'filters':32, 'kernel_size':8, 'strides':4, 'activation':'relu', 
#     'padding':'same'}),
#   (@tf.keras.layers.Conv2D,
#     {'filters':64, 'kernel_size':4, 'dilation_rate':2, 
#     'activation':'relu', 'padding':'same'}),
#   (@tf.keras.layers.Conv2D,
#     {'filters':64, 'kernel_size':3, 'activation':'relu', 
#     'padding':'same'}),
#   (@tf.keras.layers.UpSampling2D, {'size':4, 'interpolation':'bilinear'})
# ]
# right/nets.sequential.layers = [
#   (@tf.keras.layers.Conv2D,
#     {'filters':32, 'kernel_size':8, 'strides':4, 'activation':'relu', 
#     'padding':'same'}),
#   (@tf.keras.layers.Conv2D,
#     {'filters':64, 'kernel_size':4, 'dilation_rate':2, 
#     'activation':'relu', 'padding':'same'}),
#   (@tf.keras.layers.Conv2D,
#     {'filters':64, 'kernel_size':3, 'activation':'relu', 
#     'padding':'same'}),
#   (@tf.keras.layers.UpSampling2D, {'size':4, 'interpolation':'bilinear'})
# ]
# pos/nets.sequential.layers = [
#   (@tf.keras.layers.Conv2D, 
#     {'filters':160, 'kernel_size':13, 'activation':'relu', 
#     'padding':'same'}),
#   (@tf.keras.layers.Conv2D,
#     {'filters':1, 'kernel_size':1, 'padding':'same'}),
#   (@tf.keras.layers.Flatten, {})
# ]  

# nets.PseudoSiamFCN.left_layers = @left/nets.sequential
# nets.PseudoSiamFCN.right_layers = @right/nets.sequential
# nets.PseudoSiamFCN.pos_layers = @pos/nets.sequential

# Set optimizer parameters

# tf.keras.optimizers.schedules.ExponentialDecay.initial_learning_rate = 0.001
# tf.keras.optimizers.schedules.ExponentialDecay.decay_steps = 100000
# tf.keras.optimizers.schedules.ExponentialDecay.decay_rate = 0.1
# tf.keras.optimizers.schedules.ExponentialDecay.staircase = False
# tf.keras.optimizers.Adam.learning_rate = @tf.keras.optimizers.schedules.ExponentialDecay()

# tf.keras.optimizers.Adam.learning_rate = 0.001
# tf.keras.optimizers.Adam.beta_1 = 0.9
# tf.keras.optimizers.Adam.beta_2 = 0.999
# tf.keras.optimizers.Adam.epsilon = 0.0000001
# tf.keras.optimizers.Adam.amsgrad = False

# Set agent parameters

# agents.DQN.optimizer='adam'
# agents.DQN.learning_rate=None
# agents.DQN.huber_delta=1.
# agents.DQN.minibatch_size=32
# agents.DQN.replay_memory_size=100000
# agent.DQN.prefetch=None
# agents.DQN.target_update_period=10000
# agents.DQN.discount_factor=1.
# agents.DQN.collect_batch_size=None
# agents.DQN.exploration=0.1
# agents.DQN.final_exploration=None
# agents.DQN.final_exploration_iter=None
# agents.DQN.prioritization=None
# agents.DQN.priority_bias_compensation=None
# agents.DQN.double=False
# agents.DQN.n_step=None
# agents.DQN.graph=True

# Set training parameters

# siamrl.Training.num_parallel_envs = 1
# siamrl.Training.eval_env = None
# siamrl.Training.net = @nets.PseudoSiamFCN
# siamrl.Training.agent = @agents.DQN
# siamrl.Training.train_reward_buffer_length = 10
# siamrl.Training.eval_reward_buffer_length = 10
# siamrl.Training.directory = '.'
# siamrl.Training.save_evaluated_policies = False
# siamrl.Training.log_to_file = True
# siamrl.Training.log_interval = 100
# siamrl.Training.eval_interval = 10000
# siamrl.Training.checkpoint_interval = 10000
# siamrl.Traning.goal_check_interval = 1000

# siamrl.Training.initialize.num_steps = None
# siamrl.Training.initialize.policy = None