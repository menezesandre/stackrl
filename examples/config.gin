# Set environment parameters

# envs.make.env='Stack-v0'
# envs.make.n_parallel=None

# envs.make.episode_length=40
# envs.make.urdfs=None
# envs.make.object_max_dimension=0.125
# envs.make.use_gui=False
# envs.make.simulator=None
# envs.make.sim_time_step=1/100.
# envs.make.gravity=9.8
# envs.make.num_sim_steps=None
# envs.make.velocity_threshold=0.01
# envs.make.smooth_placing = True
# envs.make.observer=None
# envs.make.observable_size_ratio=4
# envs.make.resolution_factor=5
# envs.make.max_z=1
# envs.make.rewarder=None
# envs.make.goal_size_ratio=.25
# envs.make.reward_scale=1.
# envs.make.reward_params=None
# envs.make.flat_action=True
# envs.make.dtype='float32'

# envs.make.curriculum = {'goals':[], 'urdfs':[]}

# eval/envs.make.episode_length = 40
# eval/envs.make.urdfs = 50
# eval/envs.make.curriculum = None

# Set network parameters

# left/nets.unet.filters=32
# left/nets.unet.depth=3
# left/nets.unet.out_channels=None
# left/nets.unet.out_activation=None
# left/nets.unet.kernel_initializer=None

# right/nets.unet.filters=32
# right/nets.unet.depth=3
# right/nets.unet.out_channels=None
# right/nets.unet.out_activation=None
# right/nets.unet.kernel_initializer=None

# nets.correlation.parallel_iterations=None

# nets.pos_layers.filters=32
# nets.pos_layers.depth=2

# nets.PseudoSiamFCN.left_layers = @left/nets.sequential
# nets.PseudoSiamFCN.right_layers = @right/nets.sequential
# nets.PseudoSiamFCN.pos_layers = @nets.pos_layers

# Schedules

# schedules.ExponentialDecay.initial_learning_rate = 0.001
# schedules.ExponentialDecay.decay_steps = 100000
# schedules.ExponentialDecay.decay_rate = 0.1
# schedules.ExponentialDecay.staircase = False

# schedules.PolynomialDecay.initial_learning_rate = 0.001
# schedules.PolynomialDecay.decay_steps = 100000
# schedules.PolynomialDecay.end_learning_rate = 0.0001
# schedules.PolynomialDecay.power = 1.0

# Set optimizer parameters

# optimizers.Adam.learning_rate = 0.001
# optimizers.Adam.learning_rate = @lr/tf.keras.optimizers.schedules.ExponentialDecay()
# optimizers.Adam.beta_1 = 0.9
# optimizers.Adam.beta_2 = 0.999
# optimizers.Adam.epsilon = 0.0000001
# optimizers.Adam.amsgrad = False

# Set agent parameters

# agents.DQN.optimizer='adam'
# agents.DQN.learning_rate=None
# agents.DQN.huber_delta=1.
# agents.DQN.minibatch_size=32
# agents.DQN.replay_memory_size=100000
# agents.DQN.prefetch=None
# agents.DQN.target_update_period=10000
# agents.DQN.discount_factor=.99
# agents.DQN.collect_batch_size=None
# agents.DQN.exploration=0.1
# agents.DQN.prioritization=None
# agents.DQN.priority_bias_compensation=None
# agents.DQN.double=False
# agents.DQN.n_step=None
# agents.DQN.graph=True

# Set training parameters

# siamrl.Training.env = @envs.make
# siamrl.Training.eval_env = @eval/envs.make
# siamrl.Training.net = @nets.PseudoSiamFCN
# siamrl.Training.agent = @agents.DQN
# siamrl.Training.train_reward_buffer_length = 10
# siamrl.Training.eval_reward_buffer_length = 10
# siamrl.Training.directory = '.'
# siamrl.Training.save_evaluated_policies = False
# siamrl.Training.log_to_file = True
# siamrl.Training.log_interval = 100
# siamrl.Training.eval_interval = 10000
# siamrl.Training.checkpoint_interval = 10000
# siamrl.Traning.goal_check_interval = 1000

# siamrl.Training.initialize.num_steps = None
# siamrl.Training.initialize.policy = None

# siamrl.Training.run.max_num_iters = 1000000
# siamrl.Training.run.stop_when_complete = False